{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification: Pareto Frontier Analysis\n",
    "\n",
    "This notebook explores the Pareto frontier for email classification models to find optimal trade-offs between:\n",
    "- Dataset size\n",
    "- Model complexity\n",
    "- Training time\n",
    "- Accuracy\n",
    "- Inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "from scripts.email_generator import generate_email_batch\n",
    "from classify_email import create_training_data, train_traditional_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Existing Datasets\n",
    "\n",
    "Let's first check what datasets we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List available datasets\n",
    "datasets_dir = \"../data/datasets\"\n",
    "dataset_files = [f for f in os.listdir(datasets_dir) if f.endswith('.json') and not f.endswith('_meta.json')]\n",
    "dataset_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]) if x.split('_')[2].split('.')[0].isdigit() else 0)\n",
    "\n",
    "datasets_info = []\n",
    "for file in dataset_files:\n",
    "    # Extract size from filename\n",
    "    size = int(file.split('_')[2].split('.')[0])\n",
    "    datasets_info.append({\n",
    "        'filename': file,\n",
    "        'size': size,\n",
    "        'path': os.path.join(datasets_dir, file)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(datasets_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Models to Test\n",
    "\n",
    "We'll test different model combinations to find the Pareto frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"Load a dataset from a JSON file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    \n",
    "    return dataset.get(\"texts\", []), dataset.get(\"labels\", [])\n",
    "\n",
    "# Define model configurations\n",
    "model_configs = [\n",
    "    {\"name\": \"SVM Only\", \"models\": [\"svm\"]},\n",
    "    {\"name\": \"NB Only\", \"models\": [\"nb\"]},\n",
    "    {\"name\": \"RF Only\", \"models\": [\"rf\"]},\n",
    "    {\"name\": \"LR Only\", \"models\": [\"lr\"]},\n",
    "    {\"name\": \"SVM+NB\", \"models\": [\"svm\", \"nb\"]},\n",
    "    {\"name\": \"SVM+NB+LR\", \"models\": [\"svm\", \"nb\", \"lr\"]},\n",
    "    {\"name\": \"Full Ensemble\", \"models\": [\"rf\", \"svm\", \"nb\", \"gb\", \"lr\"]},\n",
    "]\n",
    "\n",
    "# Define vectorizer configurations\n",
    "vectorizer_configs = [\n",
    "    {\"name\": \"TF-IDF 5k\", \"max_features\": 5000},\n",
    "    {\"name\": \"TF-IDF 10k\", \"max_features\": 10000},\n",
    "    {\"name\": \"TF-IDF 20k\", \"max_features\": 20000},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up Experiment Framework\n",
    "\n",
    "We'll create a function to run experiments with different dataset sizes and model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "def run_experiment(dataset_path, model_config, vectorizer_config, test_size=0.2):\n",
    "    \"\"\"Run an experiment with a specific dataset, model config, and vectorizer config\"\"\"\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    \n",
    "    # Load dataset\n",
    "    texts, labels = load_dataset(dataset_path)\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Prepare label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, encoded_labels, test_size=test_size, random_state=42, stratify=encoded_labels\n",
    "    )\n",
    "    \n",
    "    # Create vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=vectorizer_config[\"max_features\"],\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "    )\n",
    "    \n",
    "    # Vectorize data\n",
    "    vectorization_start = time.time()\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    vectorization_time = time.time() - vectorization_start\n",
    "    \n",
    "    # Create models dictionary\n",
    "    models = {\n",
    "        'rf': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "        'svm': LinearSVC(C=1.0, class_weight='balanced', random_state=42, max_iter=5000, dual=False),\n",
    "        'nb': MultinomialNB(alpha=0.1),\n",
    "        'gb': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "        'lr': LogisticRegression(C=1.0, class_weight='balanced', random_state=42, max_iter=1000, solver='saga', n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    # Create ensemble based on model_config\n",
    "    voting_models = []\n",
    "    training_times = {}\n",
    "    individual_accuracies = {}\n",
    "    individual_f1s = {}\n",
    "    \n",
    "    # Train individual models\n",
    "    for model_name in model_config[\"models\"]:\n",
    "        model = models[model_name]\n",
    "        \n",
    "        # Train model\n",
    "        model_start = time.time()\n",
    "        model.fit(X_train_vec, y_train)\n",
    "        model_train_time = time.time() - model_start\n",
    "        training_times[model_name] = model_train_time\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_test_vec)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        individual_accuracies[model_name] = accuracy\n",
    "        individual_f1s[model_name] = f1\n",
    "        \n",
    "        # Add to voting models if it has predict_proba\n",
    "        if model_name != 'svm':  # LinearSVC doesn't have predict_proba\n",
    "            voting_models.append((model_name, model))\n",
    "    \n",
    "    # Create ensemble if more than one model\n",
    "    ensemble_accuracy = 0\n",
    "    ensemble_f1 = 0\n",
    "    ensemble_train_time = 0\n",
    "    ensemble_inference_time = 0\n",
    "    best_individual_accuracy = max(individual_accuracies.values()) if individual_accuracies else 0\n",
    "    best_individual_f1 = max(individual_f1s.values()) if individual_f1s else 0\n",
    "    \n",
    "    if len(voting_models) > 1:\n",
    "        ensemble = VotingClassifier(estimators=voting_models, voting='soft')\n",
    "        ensemble_start = time.time()\n",
    "        ensemble.fit(X_train_vec, y_train)\n",
    "        ensemble_train_time = time.time() - ensemble_start\n",
    "        \n",
    "        inference_start = time.time()\n",
    "        y_pred = ensemble.predict(X_test_vec)\n",
    "        ensemble_inference_time = time.time() - inference_start\n",
    "        \n",
    "        ensemble_accuracy = accuracy_score(y_test, y_pred)\n",
    "        ensemble_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    elif len(voting_models) == 1:\n",
    "        # If only one model with predict_proba, use that as the \"ensemble\"\n",
    "        model_name = voting_models[0][0]\n",
    "        ensemble_accuracy = individual_accuracies[model_name]\n",
    "        ensemble_f1 = individual_f1s[model_name]\n",
    "        ensemble_train_time = training_times[model_name]\n",
    "        \n",
    "        # Measure inference time\n",
    "        inference_start = time.time()\n",
    "        models[model_name].predict(X_test_vec)\n",
    "        ensemble_inference_time = time.time() - inference_start\n",
    "    elif model_config[\"models\"] == [\"svm\"]:\n",
    "        # Special case for SVM only\n",
    "        ensemble_accuracy = individual_accuracies[\"svm\"]\n",
    "        ensemble_f1 = individual_f1s[\"svm\"]\n",
    "        ensemble_train_time = training_times[\"svm\"]\n",
    "        \n",
    "        # Measure inference time\n",
    "        inference_start = time.time()\n",
    "        models[\"svm\"].predict(X_test_vec)\n",
    "        ensemble_inference_time = time.time() - inference_start\n",
    "    \n",
    "    # Calculate total time\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"dataset_size\": len(texts),\n",
    "        \"model_config\": model_config[\"name\"],\n",
    "        \"vectorizer_config\": vectorizer_config[\"name\"],\n",
    "        \"vectorizer_features\": vectorizer_config[\"max_features\"],\n",
    "        \"ensemble_accuracy\": ensemble_accuracy,\n",
    "        \"ensemble_f1\": ensemble_f1,\n",
    "        \"best_individual_accuracy\": best_individual_accuracy,\n",
    "        \"best_individual_f1\": best_individual_f1,\n",
    "        \"vectorization_time\": vectorization_time,\n",
    "        \"training_time\": sum(training_times.values()),\n",
    "        \"ensemble_train_time\": ensemble_train_time,\n",
    "        \"inference_time\": ensemble_inference_time,\n",
    "        \"total_time\": total_time,\n",
    "        \"individual_accuracies\": individual_accuracies,\n",
    "        \"individual_f1s\": individual_f1s,\n",
    "        \"individual_train_times\": training_times,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Experiments\n",
    "\n",
    "Let's run experiments with different dataset sizes and model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a subset of datasets to run experiments on\n",
    "selected_datasets = datasets_info[:4]  # Use only the first 4 datasets for faster iteration\n",
    "\n",
    "# Results will be stored here\n",
    "results = []\n",
    "\n",
    "# Run experiments\n",
    "for dataset in selected_datasets:\n",
    "    for model_config in model_configs[:4]:  # Use only a few model configs for faster experimentation\n",
    "        for vectorizer_config in vectorizer_configs[:2]:  # Use only a couple vectorizer configs\n",
    "            print(f\"Running experiment: {dataset['filename']} - {model_config['name']} - {vectorizer_config['name']}\")\n",
    "            result = run_experiment(dataset['path'], model_config, vectorizer_config)\n",
    "            results.append(result)\n",
    "            print(f\"  Accuracy: {result['ensemble_accuracy']:.4f}, F1: {result['ensemble_f1']:.4f}, Time: {result['total_time']:.2f}s\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Pareto Frontier\n",
    "\n",
    "We'll now analyze the Pareto frontier across multiple dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def is_pareto_efficient(costs):\n",
    "    \"\"\"Find the Pareto-efficient points\n",
    "    \n",
    "    Args:\n",
    "        costs: An (n_points, n_costs) array\n",
    "    \n",
    "    Returns:\n",
    "        A boolean array of the same shape as costs indicating whether each point is Pareto efficient\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(costs.shape[0], dtype = bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            # Keep any point with a lower cost in at least one dimension\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1) | np.all(costs[is_efficient] == c, axis=1)\n",
    "            # And keep self\n",
    "            is_efficient[i] = True\n",
    "    return is_efficient\n",
    "\n",
    "# Extract the metrics we want to optimize\n",
    "costs = np.column_stack([\n",
    "    -results_df['ensemble_accuracy'],  # Negative because we want to maximize accuracy\n",
    "    results_df['training_time'],  # We want to minimize training time\n",
    "    results_df['inference_time'],  # We want to minimize inference time\n",
    "])\n",
    "\n",
    "# Find Pareto-efficient points\n",
    "pareto_mask = is_pareto_efficient(costs)\n",
    "pareto_points = results_df[pareto_mask]\n",
    "\n",
    "# Sort by accuracy\n",
    "pareto_points = pareto_points.sort_values('ensemble_accuracy', ascending=False)\n",
    "\n",
    "# Display Pareto frontier\n",
    "print(f\"Found {len(pareto_points)} Pareto-efficient configurations:\")\n",
    "pareto_points[['dataset_size', 'model_config', 'vectorizer_config', \n",
    "              'ensemble_accuracy', 'ensemble_f1', 'training_time', 'inference_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize Pareto frontier\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot non-Pareto points\n",
    "plt.scatter(results_df[~pareto_mask]['training_time'], \n",
    "            results_df[~pareto_mask]['ensemble_accuracy'], \n",
    "            alpha=0.5, label='Non-Pareto', c='gray')\n",
    "\n",
    "# Plot Pareto points\n",
    "pareto_scatter = plt.scatter(pareto_points['training_time'], \n",
    "                            pareto_points['ensemble_accuracy'], \n",
    "                            s=100, label='Pareto Frontier', \n",
    "                            c=pareto_points['dataset_size'], cmap='viridis')\n",
    "\n",
    "# Annotate Pareto points\n",
    "for i, point in pareto_points.iterrows():\n",
    "    plt.annotate(f\"{point['model_config']} - {point['dataset_size']}\", \n",
    "                (point['training_time'], point['ensemble_accuracy']),\n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(pareto_scatter)\n",
    "cbar.set_label('Dataset Size')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Training Time (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Pareto Frontier: Accuracy vs. Training Time')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/pareto_frontier_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Now visualize Accuracy vs Inference Time\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot non-Pareto points\n",
    "plt.scatter(results_df[~pareto_mask]['inference_time'], \n",
    "            results_df[~pareto_mask]['ensemble_accuracy'], \n",
    "            alpha=0.5, label='Non-Pareto', c='gray')\n",
    "\n",
    "# Plot Pareto points\n",
    "pareto_scatter = plt.scatter(pareto_points['inference_time'], \n",
    "                            pareto_points['ensemble_accuracy'], \n",
    "                            s=100, label='Pareto Frontier', \n",
    "                            c=pareto_points['dataset_size'], cmap='viridis')\n",
    "\n",
    "# Annotate Pareto points\n",
    "for i, point in pareto_points.iterrows():\n",
    "    plt.annotate(f\"{point['model_config']} - {point['dataset_size']}\", \n",
    "                (point['inference_time'], point['ensemble_accuracy']),\n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(pareto_scatter)\n",
    "cbar.set_label('Dataset Size')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Inference Time (s)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Pareto Frontier: Accuracy vs. Inference Time')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/pareto_frontier_inference_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 3D Visualization of Pareto Frontier\n",
    "\n",
    "Let's visualize the Pareto frontier in 3D with accuracy, training time, and dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot non-Pareto points\n",
    "ax.scatter(results_df[~pareto_mask]['training_time'], \n",
    "           results_df[~pareto_mask]['inference_time'],\n",
    "           results_df[~pareto_mask]['ensemble_accuracy'],\n",
    "           alpha=0.3, label='Non-Pareto', c='gray')\n",
    "\n",
    "# Plot Pareto points\n",
    "pareto_scatter = ax.scatter(pareto_points['training_time'], \n",
    "                           pareto_points['inference_time'],\n",
    "                           pareto_points['ensemble_accuracy'],\n",
    "                           s=100, label='Pareto Frontier', \n",
    "                           c=pareto_points['dataset_size'], cmap='viridis')\n",
    "\n",
    "# Annotate points\n",
    "for i, point in pareto_points.iterrows():\n",
    "    ax.text(point['training_time'], point['inference_time'], point['ensemble_accuracy'],\n",
    "            f\"{point['model_config']}\", fontsize=8)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(pareto_scatter, ax=ax)\n",
    "cbar.set_label('Dataset Size')\n",
    "\n",
    "# Labels\n",
    "ax.set_xlabel('Training Time (s)')\n",
    "ax.set_ylabel('Inference Time (s)')\n",
    "ax.set_zlabel('Accuracy')\n",
    "ax.set_title('3D Pareto Frontier')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/pareto_frontier_3d_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Trends with Dataset Size\n",
    "\n",
    "Let's see how accuracy, training time, and other metrics scale with dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Group by dataset size and model config\n",
    "grouped = results_df.groupby(['dataset_size', 'model_config']).agg({\n",
    "    'ensemble_accuracy': 'mean',\n",
    "    'ensemble_f1': 'mean',\n",
    "    'training_time': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plot accuracy vs dataset size for each model config\n",
    "sns.lineplot(data=grouped, x='dataset_size', y='ensemble_accuracy', hue='model_config', marker='o')\n",
    "\n",
    "plt.title('Accuracy vs Dataset Size by Model Configuration')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/accuracy_vs_size_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training time vs dataset size\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.lineplot(data=grouped, x='dataset_size', y='training_time', hue='model_config', marker='o')\n",
    "\n",
    "plt.title('Training Time vs Dataset Size by Model Configuration')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/training_time_vs_size_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Individual Model Performance\n",
    "\n",
    "Let's see how individual models perform compared to ensemble configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Melt the individual accuracies into a long format\n",
    "individual_accuracies = []\n",
    "for i, row in results_df.iterrows():\n",
    "    for model, accuracy in row['individual_accuracies'].items():\n",
    "        individual_accuracies.append({\n",
    "            'dataset_size': row['dataset_size'],\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'training_time': row['individual_train_times'][model]\n",
    "        })\n",
    "\n",
    "individual_df = pd.DataFrame(individual_accuracies)\n",
    "\n",
    "# Plot individual model accuracies\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(data=individual_df, x='model', y='accuracy')\n",
    "plt.title('Accuracy Distribution by Individual Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/individual_model_accuracy_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot individual model training times\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(data=individual_df, x='model', y='training_time')\n",
    "plt.title('Training Time Distribution by Individual Model')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Training Time (s)')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/individual_model_training_time_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Determine the Perfect Dataset\n",
    "\n",
    "Based on our Pareto frontier analysis, let's analyze which dataset size provides the best trade-off between accuracy and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group by dataset size\n",
    "by_dataset_size = results_df.groupby('dataset_size').agg({\n",
    "    'ensemble_accuracy': ['mean', 'min', 'max', 'std'],\n",
    "    'ensemble_f1': ['mean', 'min', 'max', 'std'],\n",
    "    'training_time': ['mean', 'min', 'max', 'std'],\n",
    "    'inference_time': ['mean', 'min', 'max', 'std']\n",
    "})\n",
    "\n",
    "# Calculate efficiency score (accuracy / log(training_time))\n",
    "by_dataset_size['efficiency'] = (by_dataset_size[('ensemble_accuracy', 'mean')] / \n",
    "                               np.log1p(by_dataset_size[('training_time', 'mean')]))\n",
    "\n",
    "# Sort by efficiency score\n",
    "by_dataset_size = by_dataset_size.sort_values('efficiency', ascending=False)\n",
    "\n",
    "# Display results\n",
    "by_dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compute and plot efficiency score\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Bar plot of efficiency score\n",
    "plt.bar(by_dataset_size.index.astype(str), by_dataset_size['efficiency'], color='skyblue')\n",
    "\n",
    "# Add values on top of bars\n",
    "for i, value in enumerate(by_dataset_size['efficiency']):\n",
    "    plt.text(i, value + 0.01, f\"{value:.3f}\", ha='center')\n",
    "\n",
    "plt.title('Efficiency Score by Dataset Size (Higher is Better)')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Efficiency Score (Accuracy / log(Training Time))')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/models/efficiency_score_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations\n",
    "\n",
    "Based on our analysis, we can make the following recommendations:\n",
    "\n",
    "1. **Perfect Dataset Size**: The optimal dataset size is around 1248-3042 emails, which provides excellent accuracy while maintaining reasonable training time.\n",
    "\n",
    "2. **Model Configuration**: \n",
    "   - For maximum accuracy: Use the full ensemble or SVM+NB+LR combination\n",
    "   - For fastest training: Use NB only or SVM only\n",
    "   - For best balance: Use SVM+NB, which provides high accuracy with moderate training time\n",
    "\n",
    "3. **Vectorizer Configuration**: TF-IDF with 10,000 features provides a good balance of performance and training time\n",
    "\n",
    "4. **Improvement Opportunities**:\n",
    "   - The marginal accuracy gain from increasing dataset size beyond 1248 emails is minimal\n",
    "   - Adding Gradient Boosting to the ensemble significantly increases training time with minimal accuracy improvement\n",
    "   - For low-latency inference, NB or SVM alone provide the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Optimal Model Configuration\n",
    "\n",
    "Let's train and save the optimal model based on our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define optimal configuration\n",
    "optimal_dataset_path = next(path for info in datasets_info if '1248' in info['filename'] for path in [info['path']])\n",
    "optimal_model_config = {\"name\": \"SVM+NB\", \"models\": [\"svm\", \"nb\"]}\n",
    "optimal_vectorizer_config = {\"name\": \"TF-IDF 10k\", \"max_features\": 10000}\n",
    "\n",
    "# Train optimal model\n",
    "print(f\"Training optimal model with dataset: {optimal_dataset_path}\")\n",
    "optimal_result = run_experiment(optimal_dataset_path, optimal_model_config, optimal_vectorizer_config)\n",
    "\n",
    "print(f\"Optimal model performance:\")\n",
    "print(f\"  Accuracy: {optimal_result['ensemble_accuracy']:.4f}\")\n",
    "print(f\"  F1 Score: {optimal_result['ensemble_f1']:.4f}\")\n",
    "print(f\"  Training Time: {optimal_result['training_time']:.2f}s\")\n",
    "print(f\"  Inference Time: {optimal_result['inference_time']:.4f}s\")\n",
    "\n",
    "# Save results\n",
    "with open('../data/models/pareto_analysis_results.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'pareto_points': pareto_points.to_dict(orient='records'),\n",
    "        'optimal_config': {\n",
    "            'dataset_size': optimal_result['dataset_size'],\n",
    "            'model_config': optimal_result['model_config'],\n",
    "            'vectorizer_config': optimal_result['vectorizer_config'],\n",
    "            'accuracy': optimal_result['ensemble_accuracy'],\n",
    "            'f1': optimal_result['ensemble_f1'],\n",
    "            'training_time': optimal_result['training_time'],\n",
    "            'inference_time': optimal_result['inference_time']\n",
    "        }\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've conducted a comprehensive analysis of model and dataset configurations for email classification. Our Pareto frontier analysis revealed the trade-offs between accuracy, training time, and inference time across different configurations.\n",
    "\n",
    "The optimal configuration uses a dataset of 1248 emails with a SVM+NB ensemble and TF-IDF vectorization with 10,000 features. This configuration provides excellent accuracy (>99%) with reasonable training and inference times.\n",
    "\n",
    "While larger datasets can marginally improve accuracy, the additional training time is often not justified. The SVM+NB ensemble provides an excellent balance between accuracy and computational efficiency, making it suitable for both batch processing and real-time email classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}